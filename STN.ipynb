{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNZyp/bdxM6knzrkHG7QMmR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syoung7388/Traffic_sign_recognition/blob/main/STN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spatial transformer networks**\n",
        "#####  이미지의 관심 영역을 잘라내거나, 크기를 조정하거나, 방향을 수정하는 것이다. 이러한 회전, 크기 조정 등의 일반적인 아핀(Affine) 변환된 입력에 대해 결과의 변동이 크기 때문에, STN은 이를 극복하는데 매우 유용한 메커니즘이다. STN이 가진 장점중 하나는 아주 작은 수정만으로 기존에 사용하던 CNN에 간단하게 연결 시킬 수 있다는 것이다."
      ],
      "metadata": {
        "id": "Cvi5H--06EDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import here\n",
        "from google.colab import drive\n",
        "import os \n",
        "import PIL\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader \n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid\n",
        "import torch.utils.data.sampler as sampler\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv7EupRa9zzu",
        "outputId": "b7a819f5-48e4-4b5f-a98e-c90a8ea3c516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Stn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Stn, self).__init__()\n",
        "    self.loc_net = nn.Sequential(\n",
        "        nn.Conv2d(1, 50, 7), \n",
        "        nn.MaxPool2d(2, 2), \n",
        "        nn.ELU(), \n",
        "        nn.Conv2d(50, 100, 5), \n",
        "        nn.MaxPool2d(2, 2), \n",
        "        nn.ELU()\n",
        "    )\n",
        "\n",
        "    self.fc_loc = nn.Sequential(\n",
        "        nn.Linear(100*4*4, 100), \n",
        "        nn.ELU(), \n",
        "        nn.Linear(100, 3*2)\n",
        "    )\n",
        "\n",
        "    self.fc_loc[2].weight.data.zero_()\n",
        "    self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype = torch.float))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      xs = self.loc_net(x)\n",
        "      xs = xs.view(-1, 100*4*4)\n",
        "      theta = self.fc_loc(xs)\n",
        "      theta = theta.view(-1, 2, 3)\n",
        "\n",
        "      grid = F.affine_grid(theta, x.size())\n",
        "      x = F.grid_sample(x, grid)\n",
        "      return x\n",
        "\n",
        "class TrafficSignNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TrafficSignNet, self).__init__()\n",
        "\n",
        "    self.stn = Stn()\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(1, 100, 5)\n",
        "    self.conv1_bn = nn.BatchNorm2d(100) # 정규화를 입력에 적용하여 평균 및 단위 분산이 0이 되고 네트워크 정확도를 높입니다.\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    \n",
        "    self.conv2 = nn.Conv2d(100, 150, 3)\n",
        "    self.conv2_bn = nn.BatchNorm2d(150)\n",
        "    \n",
        "    self.conv3 = nn.Conv2d(150, 250, 1)\n",
        "    self.conv3_bn = nn.BatchNorm2d(250)\n",
        "\n",
        "\n",
        "    self.fc1 = nn.Linear(250*3*3, 350)\n",
        "    self.fc1_bn = nn.BatchNorm1d(350)\n",
        "    \n",
        "    self.fc2 = nn.Linear(350, 43)\n",
        "\n",
        "\n",
        "    self.dropout = nn.Dropout(p = 0.5)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.pool(F.elu(self.conv1(x)))\n",
        "      x = self.dropout(self.conv1_bn(x))\n",
        "      x = self.pool(F.elu(self.conv2(x)))\n",
        "      x = self.dropout(self.conv2_bn(x))\n",
        "      x = self.pool(F.elu(self.conv3(x)))\n",
        "      x = self.dropout(self.conv3_bn(x))\n",
        "      x = x.view(-1, 250 * 3 * 3)\n",
        "      x = F.elu(self.fc1(x))\n",
        "      x = self.dropout(self.fc1_bn(x))\n",
        "      x = self.fc2(x)\n",
        "      return x\n",
        "    "
      ],
      "metadata": {
        "id": "J5q6qrga69Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBM_LTIKofW2"
      },
      "outputs": [],
      "source": [
        "def loss_batch(model, loss_func, x, y, opt = None):\n",
        "    loss = loss_func(model(x), y)\n",
        "    if opt is not None:\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    return loss.item(), len(x)\n",
        "def valid_batch(model, loss_func, x, y):\n",
        "    output = model(x)\n",
        "    loss = loss_func(output, y)\n",
        "    pred = torch.argmax(output, dim = 1)\n",
        "    correct = pred == y.view(*pred.shape)\n",
        "    return loss.item(), torch.sum(correct).item(), len(x)\n",
        "\n",
        "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        #Train model\n",
        "        model.train()\n",
        "        losses, nums = zip(*[loss_batch(model, loss_func, x, y, opt) for x, y in train_dl])\n",
        "        train_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "        \n",
        "        #Validation model\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            losses, corrects, nums = zip(*[valid_batch(model, loss_func, x, y) for x, y in valid_dl])\n",
        "            valid_loss = np.sum(np.multiply(losses, nums) / np.sum(nums))\n",
        "            valid_accuracy = np.sum(corrects) / np.sum(nums)*100\n",
        "            print(f\"[Epoch {epoch+1}/{epochs}] \"\n",
        "                  f\"Train loss: {train_loss:.6f}\\t\"\n",
        "                  f\"Validation loss: {valid_loss:.6f}\\t\",\n",
        "                  f\"Validation accruacy: {valid_accuracy:.3f}%\")            \n",
        "def evaluate(model, loss_func, dl):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        losses, corrects, nums = zip(*[valid_batch(model, loss_func, x, y) for x, y in dl])\n",
        "        test_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "        test_accuracy = np.sum(corrects) / np.sum(nums) * 100\n",
        "        \n",
        "    print(f\"Test loss: {test_loss:.6f}\\t\"\n",
        "          f\"Test accruacy: {test_accuracy:.3f}%\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Custom DataSet & DataLoader \n",
        "class PickledDataset(Dataset):\n",
        "    def __init__(self, file_path, transform = None):\n",
        "        with open(file_path, mode = 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            self.features = data['features']\n",
        "            self.labels = data['labels']\n",
        "            self.count = len(self.labels)\n",
        "            self.transform = transform\n",
        "    def __getitem__(self, index):\n",
        "        feature = self.features[index]\n",
        "        if self.transform is not None:\n",
        "            feature = self.transform(feature)\n",
        "        return (feature, self.labels[index])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.count\n",
        "    \n",
        "class WrappendDataLoader:\n",
        "    def __init__(self, dl, func):\n",
        "        self.dl = dl\n",
        "        self.func = func\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "    \n",
        "    def __iter__(self):\n",
        "        batches = iter(self.dl)\n",
        "        for b in batches:\n",
        "            yield (self.func(*b))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Load Data\n",
        "\n",
        "\n",
        "training_file = '/content/drive/MyDrive/Traffic/data/train_gray.p'\n",
        "validation_file = '/content/drive/MyDrive/Traffic/data/valid_gray.p'\n",
        "testing_file = '/content/drive/MyDrive/Traffic/data/test_gray.p'\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def to_device(x, y):\n",
        "    return x.to(device), y.to(device, dtype = torch.int64)\n",
        "\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.ToTensor()                                      \n",
        "])\n",
        "train_dataset = PickledDataset(training_file, transform =data_transforms )\n",
        "valid_dataset = PickledDataset(validation_file, transform = data_transforms)\n",
        "test_dataset = PickledDataset(testing_file, transform = data_transforms)\n",
        "\n",
        "\n",
        "\n",
        "n_epochs = 20\n",
        "n_train = len(train_dataset.features)\n",
        "n_classes = len(set(train_dataset.labels))\n",
        "\n",
        "train_loader = WrappendDataLoader(DataLoader(train_dataset, batch_size = 64, shuffle = True), to_device)\n",
        "valid_loader = WrappendDataLoader(DataLoader(valid_dataset, batch_size = 64, shuffle = False),to_device)\n",
        "test_loader = WrappendDataLoader(DataLoader(test_dataset, batch_size = 64, shuffle = False),to_device)\n",
        "\n"
      ],
      "metadata": {
        "id": "rD25e5ul966W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Train model \n",
        "\n",
        "model = TrafficSignNet().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "fit(n_epochs, model, criterion, optimizer, train_loader, valid_loader) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j84RVeZS89dN",
        "outputId": "6155ffe3-4402-4474-a282-b926573986b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/20] Train loss: 2.276792\tValidation loss: 1.320466\t Validation accruacy: 64.150%\n",
            "[Epoch 2/20] Train loss: 1.047861\tValidation loss: 0.738623\t Validation accruacy: 80.930%\n",
            "[Epoch 3/20] Train loss: 0.657835\tValidation loss: 0.462388\t Validation accruacy: 87.143%\n",
            "[Epoch 4/20] Train loss: 0.450904\tValidation loss: 0.317950\t Validation accruacy: 91.791%\n",
            "[Epoch 5/20] Train loss: 0.337887\tValidation loss: 0.240050\t Validation accruacy: 93.946%\n",
            "[Epoch 6/20] Train loss: 0.261630\tValidation loss: 0.199844\t Validation accruacy: 95.306%\n",
            "[Epoch 7/20] Train loss: 0.215207\tValidation loss: 0.173271\t Validation accruacy: 95.669%\n",
            "[Epoch 8/20] Train loss: 0.174723\tValidation loss: 0.142439\t Validation accruacy: 96.168%\n",
            "[Epoch 9/20] Train loss: 0.152137\tValidation loss: 0.132627\t Validation accruacy: 96.349%\n",
            "[Epoch 10/20] Train loss: 0.130859\tValidation loss: 0.125999\t Validation accruacy: 96.485%\n",
            "[Epoch 11/20] Train loss: 0.116451\tValidation loss: 0.105677\t Validation accruacy: 96.916%\n",
            "[Epoch 12/20] Train loss: 0.103120\tValidation loss: 0.105590\t Validation accruacy: 97.075%\n",
            "[Epoch 13/20] Train loss: 0.096185\tValidation loss: 0.104265\t Validation accruacy: 97.075%\n",
            "[Epoch 14/20] Train loss: 0.085394\tValidation loss: 0.096315\t Validation accruacy: 97.324%\n",
            "[Epoch 15/20] Train loss: 0.081270\tValidation loss: 0.100122\t Validation accruacy: 97.347%\n",
            "[Epoch 16/20] Train loss: 0.072880\tValidation loss: 0.084248\t Validation accruacy: 97.324%\n",
            "[Epoch 17/20] Train loss: 0.067910\tValidation loss: 0.082347\t Validation accruacy: 97.574%\n",
            "[Epoch 18/20] Train loss: 0.062613\tValidation loss: 0.088801\t Validation accruacy: 97.574%\n",
            "[Epoch 19/20] Train loss: 0.061037\tValidation loss: 0.083942\t Validation accruacy: 97.619%\n",
            "[Epoch 20/20] Train loss: 0.054556\tValidation loss: 0.080903\t Validation accruacy: 97.596%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model\n",
        "evaluate(model, criterion, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiKkfzyj9WW4",
        "outputId": "fc8034c5-b4aa-4f83-c738-973eb50bfe25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.089112\tTest accruacy: 97.443%\n"
          ]
        }
      ]
    }
  ]
}